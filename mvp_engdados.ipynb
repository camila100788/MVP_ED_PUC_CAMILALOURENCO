{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1b0fa7f-0206-4454-acc0-6bb6f1dc59c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# IMPORTAÇÕES\n",
    "# ===========================================================\n",
    "import requests\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import io\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, row_number\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# ===========================================================\n",
    "# CRIAR SESSÃO SPARK\n",
    "# ===========================================================\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ANP Preços Combustíveis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# ===========================================================\n",
    "# BAIXAR E CARREGAR OS DADOS DA ANP (ZIP → CSV)\n",
    "# ===========================================================\n",
    "url_zip = \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos/shpc/dsas/ca/ca-2025-01.zip\"\n",
    "\n",
    "response = requests.get(url_zip)\n",
    "response.raise_for_status()\n",
    "zip_bytes = io.BytesIO(response.content)\n",
    "print(\"ZIP carregado com sucesso.\")\n",
    "\n",
    "with zipfile.ZipFile(zip_bytes) as zip_ref:\n",
    "    csv_name = [n for n in zip_ref.namelist() if n.lower().endswith(\".csv\")][0]\n",
    "    csv_bytes = zip_ref.read(csv_name)\n",
    "\n",
    "print(f\"CSV encontrado dentro do ZIP: {csv_name}\")\n",
    "\n",
    "# ===========================================================\n",
    "# LER CSV E NORMALIZAR COLUNAS\n",
    "# ===========================================================\n",
    "df = pd.read_csv(io.BytesIO(csv_bytes), sep=\";\", encoding=\"latin1\", low_memory=False)\n",
    "\n",
    "def normalize_column(col):\n",
    "    col = col.lower()\n",
    "    col = ''.join(c for c in unicodedata.normalize('NFD', col) if unicodedata.category(c) != 'Mn')\n",
    "    col = col.replace(\" \", \"_\")\n",
    "    return col\n",
    "\n",
    "df.columns = [normalize_column(c) for c in df.columns]\n",
    "df.columns = [c.replace(\"ï»¿\", \"\") for c in df.columns]\n",
    "\n",
    "print(\"Colunas normalizadas:\", df.columns)\n",
    "\n",
    "# ===========================================================\n",
    "# CONVERSÃO DE VALORES NUMÉRICOS\n",
    "# ===========================================================\n",
    "for coluna in [\"valor_de_venda\", \"valor_de_compra\"]:\n",
    "    df[coluna] = df[coluna].astype(str).str.replace(\",\", \".\", regex=False)\n",
    "    df[coluna] = pd.to_numeric(df[coluna], errors=\"coerce\")\n",
    "\n",
    "# ===========================================================\n",
    "# CRIAR DATAFRAME SPARK\n",
    "# ===========================================================\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "spark_df = spark_df.withColumn(\"valor_de_venda\",\n",
    "    F.regexp_replace(\"valor_de_venda\", \",\", \".\").cast(DoubleType())\n",
    ")\n",
    "spark_df = spark_df.withColumn(\"valor_de_compra\",\n",
    "    F.regexp_replace(\"valor_de_compra\", \",\", \".\").cast(DoubleType())\n",
    ")\n",
    "spark_df = spark_df.withColumn(\"data_da_coleta\",\n",
    "    F.to_date(\"data_da_coleta\", \"dd/MM/yyyy\")\n",
    ")\n",
    "\n",
    "# ===========================================================\n",
    "# LIMPEZA DE DADOS\n",
    "# ===========================================================\n",
    "# Remover duplicados completos\n",
    "spark_df = spark_df.dropDuplicates()\n",
    "\n",
    "# Atualizar valor_de_compra = valor_de_venda / 3\n",
    "spark_df = spark_df.withColumn(\"valor_de_compra\", col(\"valor_de_venda\") / 3)\n",
    "\n",
    "# Corrigir nomes de colunas\n",
    "for col_name in spark_df.columns:\n",
    "    if \"regiao\" in col_name:\n",
    "        spark_df = spark_df.withColumnRenamed(col_name, \"regiao_sigla\")\n",
    "    if \"estado\" in col_name:\n",
    "        spark_df = spark_df.withColumnRenamed(col_name, \"estado_sigla\")\n",
    "\n",
    "print(\"Colunas finais no Spark:\", spark_df.columns)\n",
    "\n",
    "# ===========================================================\n",
    "# DIMENSÃO TEMPO\n",
    "# ===========================================================\n",
    "dim_tempo = (\n",
    "    spark_df\n",
    "    .select(\"data_da_coleta\").distinct()\n",
    "    .withColumn(\"ano\", F.year(\"data_da_coleta\"))\n",
    "    .withColumn(\"mes\", F.month(\"data_da_coleta\"))\n",
    "    .withColumn(\"dia\", F.dayofmonth(\"data_da_coleta\"))\n",
    "    .withColumnRenamed(\"data_da_coleta\", \"data\")\n",
    "    .withColumn(\"tempo_id\", F.monotonically_increasing_id())\n",
    ")\n",
    "dim_tempo.createOrReplaceTempView(\"dim_tempo\")\n",
    "\n",
    "# ===========================================================\n",
    "# DIMENSÃO PRODUTO\n",
    "# ===========================================================\n",
    "dim_produto = (\n",
    "    spark_df\n",
    "    .select(\"produto\", \"unidade_de_medida\").distinct()\n",
    "    .withColumn(\"produto_id\", F.monotonically_increasing_id())\n",
    ")\n",
    "dim_produto.createOrReplaceTempView(\"dim_produto\")\n",
    "\n",
    "# ===========================================================\n",
    "# DIMENSÃO POSTO\n",
    "# ===========================================================\n",
    "dim_posto = (\n",
    "    spark_df.select(\n",
    "        \"cnpj_da_revenda\", \"revenda\", \"nome_da_rua\", \"numero_rua\",\n",
    "        \"complemento\", \"bairro\", \"cep\", \"municipio\",\n",
    "        \"estado_sigla\", \"regiao_sigla\", \"bandeira\"\n",
    "    )\n",
    "    .distinct()\n",
    "    .withColumnRenamed(\"cnpj_da_revenda\", \"posto_id\")\n",
    ")\n",
    "dim_posto.createOrReplaceTempView(\"dim_posto\")\n",
    "\n",
    "# ===========================================================\n",
    "# FATO PREÇOS\n",
    "# ===========================================================\n",
    "fato_precos = (\n",
    "    spark_df.alias(\"df\")\n",
    "    .join(dim_posto.alias(\"po\"), F.col(\"df.cnpj_da_revenda\") == F.col(\"po.posto_id\"), \"left\")\n",
    "    .join(dim_produto.alias(\"pr\"), F.col(\"df.produto\") == F.col(\"pr.produto\"), \"left\")\n",
    "    .join(dim_tempo.alias(\"t\"), F.col(\"df.data_da_coleta\") == F.col(\"t.data\"), \"left\")\n",
    "    .select(\n",
    "        F.col(\"po.posto_id\"),\n",
    "        F.col(\"pr.produto_id\"),\n",
    "        F.col(\"t.tempo_id\"),\n",
    "        F.col(\"df.valor_de_venda\"),\n",
    "        F.col(\"df.valor_de_compra\")\n",
    "    )\n",
    ")\n",
    "fato_precos.createOrReplaceTempView(\"fato_precos\")\n",
    "\n",
    "# ===========================================================\n",
    "# ANÁLISES E GRÁFICOS\n",
    "# ===========================================================\n",
    "\n",
    "# 1 — POSTOS MAIS RENTÁVEIS\n",
    "query1 = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        po.revenda,\n",
    "        SUM(COALESCE(f.valor_de_venda,0) - COALESCE(f.valor_de_compra,0)) AS lucro_total,\n",
    "        AVG(COALESCE(f.valor_de_venda,0) - COALESCE(f.valor_de_compra,0)) AS margem_media\n",
    "    FROM fato_precos f\n",
    "    JOIN dim_posto po ON f.posto_id = po.posto_id\n",
    "    GROUP BY po.revenda\n",
    "\"\"\")\n",
    "top_postos = query1.toPandas().sort_values(\"lucro_total\", ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=top_postos, x=\"lucro_total\", y=\"revenda\", palette=\"viridis\")\n",
    "plt.title(\"Top 10 Postos Mais Rentáveis\")\n",
    "plt.show()\n",
    "\n",
    "# ===========================================================\n",
    "# TOP 10 ESTADOS MAIS RENTÁVEIS\n",
    "# ===========================================================\n",
    "\n",
    "# Query Spark: soma do lucro por estado\n",
    "query_top_estados = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        po.estado_sigla AS estado,\n",
    "        SUM(COALESCE(f.valor_de_venda,0) - COALESCE(f.valor_de_compra,0)) AS lucro_total\n",
    "    FROM fato_precos f\n",
    "    JOIN dim_posto po ON f.posto_id = po.posto_id\n",
    "    GROUP BY po.estado_sigla\n",
    "    ORDER BY lucro_total DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "# Converter para Pandas para plot\n",
    "top_estados = query_top_estados.toPandas()\n",
    "\n",
    "# Gráfico de barras\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=top_estados, x=\"lucro_total\", y=\"estado\", palette=\"magma\")\n",
    "plt.title(\"Top 10 Estados Mais Rentáveis\")\n",
    "plt.xlabel(\"Lucro Total (R$)\")\n",
    "plt.ylabel(\"Estado\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 2 — ESTADO MAIS RENTÁVEL\n",
    "query_estado = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        po.estado_sigla AS estado,\n",
    "        SUM(COALESCE(f.valor_de_venda,0) - COALESCE(f.valor_de_compra,0)) AS lucro_total\n",
    "    FROM fato_precos f\n",
    "    JOIN dim_posto po ON f.posto_id = po.posto_id\n",
    "    GROUP BY po.estado_sigla\n",
    "    ORDER BY lucro_total DESC\n",
    "    LIMIT 1\n",
    "\"\"\")\n",
    "estado_selecionado = query_estado.toPandas().loc[0, \"estado\"]\n",
    "print(\"Estado mais lucrativo:\", estado_selecionado)\n",
    "\n",
    "# 3 — BAIRROS MAIS RENTÁVEIS DO ESTADO\n",
    "query_bairro = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        po.bairro,\n",
    "        SUM(COALESCE(f.valor_de_venda,0) - COALESCE(f.valor_de_compra,0)) AS lucro_total,\n",
    "        AVG(COALESCE(f.valor_de_venda,0) - COALESCE(f.valor_de_compra,0)) AS margem_media\n",
    "    FROM fato_precos f\n",
    "    JOIN dim_posto po ON f.posto_id = po.posto_id\n",
    "    WHERE po.estado_sigla = '{estado_selecionado}'\n",
    "    GROUP BY po.bairro\n",
    "    ORDER BY lucro_total DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "bairros_lucrativos = query_bairro.toPandas()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=bairros_lucrativos, x=\"lucro_total\", y=\"bairro\", palette=\"viridis\")\n",
    "plt.title(f\"Bairros Mais Rentáveis em {estado_selecionado}\")\n",
    "plt.show()\n",
    "\n",
    "# 4 — LUCRO POR BANDEIRA\n",
    "query_bandeira = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        po.bandeira,\n",
    "        SUM(COALESCE(f.valor_de_venda,0) - COALESCE(f.valor_de_compra,0)) AS lucro_total\n",
    "    FROM fato_precos f\n",
    "    JOIN dim_posto po ON f.posto_id = po.posto_id\n",
    "    GROUP BY po.bandeira\n",
    "\"\"\")\n",
    "bandeiras = query_bandeira.toPandas().sort_values(\"lucro_total\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(data=bandeiras, x=\"lucro_total\", y=\"bandeira\", palette=\"coolwarm\")\n",
    "plt.title(\"Lucro Total por Bandeira\")\n",
    "plt.show()\n",
    "\n",
    "# 5 — PRODUTOS: PREÇO x MARGEM\n",
    "query_produto = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        p.produto,\n",
    "        AVG(COALESCE(f.valor_de_venda,0)) AS preco_medio,\n",
    "        AVG(COALESCE(f.valor_de_venda,0) - COALESCE(f.valor_de_compra,0)) AS margem_media\n",
    "    FROM fato_precos f\n",
    "    JOIN dim_produto p ON f.produto_id = p.produto_id\n",
    "    GROUP BY p.produto\n",
    "\"\"\")\n",
    "produtos = query_produto.toPandas()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(data=produtos, x=\"preco_medio\", y=\"margem_media\", hue=\"produto\", s=120)\n",
    "plt.title(\"Preço Médio x Margem Média por Produto\")\n",
    "plt.show()\n",
    "\n",
    "# 6 — VARIAÇÃO PERCENTUAL DE PREÇOS\n",
    "query_variacao = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        po.revenda,\n",
    "        p.produto,\n",
    "        MIN(COALESCE(f.valor_de_venda,0)) AS preco_min,\n",
    "        MAX(COALESCE(f.valor_de_venda,0)) AS preco_max,\n",
    "        ( (MAX(COALESCE(f.valor_de_venda,0)) - MIN(COALESCE(f.valor_de_venda,0)))\n",
    "            / NULLIF(MIN(COALESCE(f.valor_de_venda,0)), 0) ) * 100 AS variacao_percentual\n",
    "    FROM fato_precos f\n",
    "    JOIN dim_posto po ON f.posto_id = po.posto_id\n",
    "    JOIN dim_produto p ON f.produto_id = p.produto_id\n",
    "    GROUP BY po.revenda, p.produto\n",
    "\"\"\")\n",
    "variacoes = query_variacao.toPandas().sort_values(\"variacao_percentual\", ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=variacoes, x=\"variacao_percentual\", y=\"revenda\", hue=\"produto\", dodge=False)\n",
    "plt.title(\"Top 10 Maiores Aumentos de Preço\")\n",
    "plt.show()\n",
    "\n",
    "# 7 — PRODUTOS/POSTOS COM MARGEM NEGATIVA\n",
    "query_margem_neg = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        po.revenda,\n",
    "        p.produto,\n",
    "        AVG(COALESCE(f.valor_de_venda,0)) AS preco_medio,\n",
    "        AVG(COALESCE(f.valor_de_compra,0)) AS custo_medio,\n",
    "        AVG(COALESCE(f.valor_de_venda,0) - COALESCE(f.valor_de_compra,0)) AS margem_media\n",
    "    FROM fato_precos f\n",
    "    JOIN dim_posto po ON f.posto_id = po.posto_id\n",
    "    JOIN dim_produto p ON f.produto_id = p.produto_id\n",
    "    GROUP BY po.revenda, p.produto\n",
    "    HAVING AVG(COALESCE(f.valor_de_venda,0) - COALESCE(f.valor_de_compra,0)) < 0\n",
    "\"\"\")\n",
    "margem_negativa = query_margem_neg.toPandas()\n",
    "\n",
    "if not margem_negativa.empty:\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(data=margem_negativa, x=\"margem_media\", y=\"revenda\", hue=\"produto\", palette=\"Reds_r\", dodge=False)\n",
    "    plt.title(\"Produtos/Postos com Margem Negativa\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Nenhum produto/posto com margem negativa encontrado.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "mvp_engdados",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
