{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1b0fa7f-0206-4454-acc0-6bb6f1dc59c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# IMPORTAÇÕES\n",
    "# ===========================================================\n",
    "import requests\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import io\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# ===========================================================\n",
    "# BAIXAR E CARREGAR OS DADOS DA ANP (ZIP → CSV)\n",
    "# ===========================================================\n",
    "url_zip = \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos/shpc/dsas/ca/ca-2025-01.zip\"\n",
    "\n",
    "response = requests.get(url_zip)\n",
    "response.raise_for_status()\n",
    "zip_bytes = io.BytesIO(response.content)\n",
    "\n",
    "with zipfile.ZipFile(zip_bytes) as zip_ref:\n",
    "    csv_name = [n for n in zip_ref.namelist() if n.lower().endswith(\".csv\")][0]\n",
    "    csv_bytes = zip_ref.read(csv_name)\n",
    "\n",
    "# ===========================================================\n",
    "# LER CSV E NORMALIZAR COLUNAS\n",
    "# ===========================================================\n",
    "df = pd.read_csv(io.BytesIO(csv_bytes), sep=\";\", encoding=\"latin1\", low_memory=False)\n",
    "\n",
    "def normalize_column(col):\n",
    "    col = col.lower()\n",
    "    col = ''.join(c for c in unicodedata.normalize('NFD', col) if unicodedata.category(c) != 'Mn')\n",
    "    col = col.replace(\" \", \"_\")\n",
    "    return col\n",
    "\n",
    "df.columns = [normalize_column(c) for c in df.columns]\n",
    "df.columns = [c.replace(\"ï»¿\", \"\") for c in df.columns]  # remover BOM\n",
    "\n",
    "# ===========================================================\n",
    "# CONVERTER VALORES NUMÉRICOS\n",
    "# ===========================================================\n",
    "for coluna in [\"valor_de_venda\", \"valor_de_compra\"]:\n",
    "    df[coluna] = df[coluna].astype(str).str.replace(\",\", \".\", regex=False)\n",
    "    df[coluna] = pd.to_numeric(df[coluna], errors=\"coerce\")\n",
    "\n",
    "# ===========================================================\n",
    "# CRIAR DATAFRAME SPARK\n",
    "# ===========================================================\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "spark_df = spark_df.withColumn(\"valor_de_venda\",\n",
    "    F.regexp_replace(\"valor_de_venda\", \",\", \".\").cast(DoubleType())\n",
    ")\n",
    "spark_df = spark_df.withColumn(\"valor_de_compra\",\n",
    "    F.regexp_replace(\"valor_de_compra\", \",\", \".\").cast(DoubleType())\n",
    ")\n",
    "spark_df = spark_df.withColumn(\"data_da_coleta\",\n",
    "    F.to_date(\"data_da_coleta\", \"dd/MM/yyyy\")\n",
    ")\n",
    "\n",
    "# ===========================================================\n",
    "# CORRIGIR NOMES DE COLUNAS\n",
    "# ===========================================================\n",
    "for col_name in spark_df.columns:\n",
    "    if \"regiao\" in col_name:\n",
    "        spark_df = spark_df.withColumnRenamed(col_name, \"regiao_sigla\")\n",
    "    if \"estado\" in col_name:\n",
    "        spark_df = spark_df.withColumnRenamed(col_name, \"estado_sigla\")\n",
    "\n",
    "# ===========================================================\n",
    "# LIMPEZA DE DADOS\n",
    "# ===========================================================\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, row_number\n",
    "\n",
    "window_state = Window.partitionBy(\"estado_sigla\").orderBy(\"data_da_coleta\")\n",
    "spark_df = spark_df.withColumn(\"row_num\", row_number().over(window_state))\n",
    "spark_df = spark_df.filter(col(\"row_num\") == 1).drop(\"row_num\")\n",
    "\n",
    "spark_df = spark_df.dropDuplicates()\n",
    "spark_df = spark_df.withColumn(\"valor_de_compra\", col(\"valor_de_venda\") / 3)\n",
    "\n",
    "# ===========================================================\n",
    "# DIMENSÕES\n",
    "# ===========================================================\n",
    "dim_tempo = (\n",
    "    spark_df\n",
    "    .select(\"data_da_coleta\").distinct()\n",
    "    .withColumn(\"ano\", F.year(\"data_da_coleta\"))\n",
    "    .withColumn(\"mes\", F.month(\"data_da_coleta\"))\n",
    "    .withColumn(\"dia\", F.dayofmonth(\"data_da_coleta\"))\n",
    "    .withColumnRenamed(\"data_da_coleta\", \"data\")\n",
    "    .withColumn(\"tempo_id\", F.monotonically_increasing_id())\n",
    ")\n",
    "dim_tempo.createOrReplaceTempView(\"dim_tempo\")\n",
    "\n",
    "dim_produto = (\n",
    "    spark_df\n",
    "    .select(\"produto\", \"unidade_de_medida\").distinct()\n",
    "    .withColumn(\"produto_id\", F.monotonically_increasing_id())\n",
    ")\n",
    "dim_produto.createOrReplaceTempView(\"dim_produto\")\n",
    "\n",
    "dim_posto = (\n",
    "    spark_df.select(\n",
    "        \"cnpj_da_revenda\", \"revenda\", \"nome_da_rua\", \"numero_rua\",\n",
    "        \"complemento\", \"bairro\", \"cep\", \"municipio\",\n",
    "        \"estado_sigla\", \"regiao_sigla\", \"bandeira\"\n",
    "    )\n",
    "    .distinct()\n",
    "    .withColumnRenamed(\"cnpj_da_revenda\", \"posto_id\")\n",
    ")\n",
    "dim_posto.createOrReplaceTempView(\"dim_posto\")\n",
    "\n",
    "# ===========================================================\n",
    "# FATO PREÇOS\n",
    "# ===========================================================\n",
    "fato_precos = (\n",
    "    spark_df.alias(\"df\")\n",
    "    .join(dim_posto.alias(\"po\"), F.col(\"df.cnpj_da_revenda\") == F.col(\"po.posto_id\"), \"left\")\n",
    "    .join(dim_produto.alias(\"pr\"), F.col(\"df.produto\") == F.col(\"pr.produto\"), \"left\")\n",
    "    .join(dim_tempo.alias(\"t\"), F.col(\"df.data_da_coleta\") == F.col(\"t.data\"), \"left\")\n",
    "    .select(\n",
    "        F.col(\"po.posto_id\"),\n",
    "        F.col(\"pr.produto_id\"),\n",
    "        F.col(\"t.tempo_id\"),\n",
    "        F.col(\"df.valor_de_venda\"),\n",
    "        F.col(\"df.valor_de_compra\")\n",
    "    )\n",
    ")\n",
    "fato_precos.createOrReplaceTempView(\"fato_precos\")\n",
    "\n",
    "# ===========================================================\n",
    "# ANALISES E GRÁFICOS (POSTOS, ESTADOS, BAIRROS, BANDEIRA, PRODUTOS, VARIAÇÃO)\n",
    "# ===========================================================\n",
    "\n",
    "# Top 10 postos mais rentáveis\n",
    "query1 = spark.sql(\"\"\"\n",
    "    SELECT po.revenda,\n",
    "           SUM(COALESCE(f.valor_de_venda,0) - COALESCE(f.valor_de_compra,0)) AS lucro_total,\n",
    "           AVG(COALESCE(f.valor_de_venda,0) - COALESCE(f.valor_de_compra,0)) AS margem_media\n",
    "    FROM fato_precos f\n",
    "    JOIN dim_posto po ON f.posto_id = po.posto_id\n",
    "    GROUP BY po.revenda\n",
    "\"\"\")\n",
    "top_postos = query1.toPandas().sort_values(\"lucro_total\", ascending=False).head(10)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=top_postos, x=\"lucro_total\", y=\"revenda\", palette=\"viridis\")\n",
    "plt.title(\"Top 10 Postos Mais Rentáveis\")\n",
    "plt.show()\n",
    "\n",
    "# Estado mais lucrativo\n",
    "query_estado = spark.sql(\"\"\"\n",
    "    SELECT po.estado_sigla AS estado,\n",
    "           SUM(COALESCE(f.valor_de_venda,0) - COALESCE(f.valor_de_compra,0)) AS lucro_total\n",
    "    FROM fato_precos f\n",
    "    JOIN dim_posto po ON f.posto_id = po.posto_id\n",
    "    GROUP BY po.estado_sigla\n",
    "    ORDER BY lucro_total DESC\n",
    "    LIMIT 1\n",
    "\"\"\")\n",
    "estado_selecionado = query_estado.toPandas().loc[0, \"estado\"]\n",
    "print(\"Estado mais lucrativo:\", estado_selecionado)\n",
    "\n",
    "# Top 10 bairros mais rentáveis\n",
    "query_bairro = spark.sql(f\"\"\"\n",
    "    SELECT po.bairro,\n",
    "           SUM(COALESCE(f.valor_de_venda,0) - COALESCE(f.valor_de_compra,0)) AS lucro_total,\n",
    "           AVG(COALESCE(f.valor_de_venda,0) - COALESCE(f.valor_de_compra,0)) AS margem_media\n",
    "    FROM fato_precos f\n",
    "    JOIN dim_posto po ON f.posto_id = po.posto_id\n",
    "    WHERE po.estado_sigla = '{estado_selecionado}'\n",
    "    GROUP BY po.bairro\n",
    "    ORDER BY lucro_total DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "bairros_lucrativos = query_bairro.toPandas()\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=bairros_lucrativos, x=\"lucro_total\", y=\"bairro\", palette=\"viridis\")\n",
    "plt.title(f\"Bairros Mais Rentáveis em {estado_selecionado}\")\n",
    "plt.show()\n",
    "\n",
    "# Lucro por bandeira\n",
    "query4 = spark.sql(\"\"\"\n",
    "    SELECT po.bandeira,\n",
    "           SUM(COALESCE(f.valor_de_venda,0) - COALESCE(f.valor_de_compra,0)) AS lucro_total\n",
    "    FROM fato_precos f\n",
    "    JOIN dim_posto po ON f.posto_id = po.posto_id\n",
    "    GROUP BY po.bandeira\n",
    "\"\"\")\n",
    "bandeiras = query4.toPandas().sort_values(\"lucro_total\", ascending=False)\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(data=bandeiras, x=\"lucro_total\", y=\"bandeira\", palette=\"coolwarm\")\n",
    "plt.title(\"Lucro Total por Bandeira\")\n",
    "plt.show()\n",
    "\n",
    "# Produtos — preço médio x margem média\n",
    "query5 = spark.sql(\"\"\"\n",
    "    SELECT p.produto,\n",
    "           AVG(COALESCE(f.valor_de_venda,0)) AS preco_medio,\n",
    "           AVG(COALESCE(f.valor_de_venda,0) - COALESCE(f.valor_de_compra,0)) AS margem_media\n",
    "    FROM fato_precos f\n",
    "    JOIN dim_produto p ON f.produto_id = p.produto_id\n",
    "    GROUP BY p.produto\n",
    "\"\"\")\n",
    "produtos = query5.toPandas()\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(data=produtos, x=\"preco_medio\", y=\"margem_media\", hue=\"produto\", s=120)\n",
    "plt.title(\"Preço Médio x Margem Média por Produto\")\n",
    "plt.show()\n",
    "\n",
    "# Maiores aumentos percentuais de preço\n",
    "query6 = spark.sql(\"\"\"\n",
    "    SELECT po.revenda,\n",
    "           p.produto,\n",
    "           MIN(COALESCE(f.valor_de_venda,0)) AS preco_min,\n",
    "           MAX(COALESCE(f.valor_de_venda,0)) AS preco_max,\n",
    "           ( (MAX(COALESCE(f.valor_de_venda,0)) - MIN(COALESCE(f.valor_de_venda,0)))\n",
    "            / NULLIF(MIN(COALESCE(f.valor_de_venda,0)), 0) ) * 100 AS variacao_percentual\n",
    "    FROM fato_precos f\n",
    "    JOIN dim_posto po ON f.posto_id = po.posto_id\n",
    "    JOIN dim_produto p ON f.produto_id = p.produto_id\n",
    "    GROUP BY po.revenda, p.produto\n",
    "\"\"\")\n",
    "variacoes = query6.toPandas().sort_values(\"variacao_percentual\", ascending=False).head(10)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=variacoes, x=\"variacao_percentual\", y=\"revenda\", hue=\"produto\", dodge=False)\n",
    "plt.title(\"Top 10 Maiores Aumentos de Preço\")\n",
    "plt.show()\n",
    "\n",
    "# Margem negativa\n",
    "query7 = spark.sql(\"\"\"\n",
    "    SELECT po.revenda,\n",
    "           p.produto,\n",
    "           AVG(COALESCE(f.valor_de_venda,0)) AS preco_medio,\n",
    "           AVG(COALESCE(f.valor_de_compra,0)) AS custo_medio,\n",
    "           AVG(COALESCE(f.valor_de_venda,0) - COALESCE(f.valor_de_compra,0)) AS margem_media\n",
    "    FROM fato_precos f\n",
    "    JOIN dim_posto po ON f.posto_id = po.posto_id\n",
    "    JOIN dim_produto p ON f.produto_id = p.produto_id\n",
    "    GROUP BY po.revenda, p.produto\n",
    "    HAVING AVG(COALESCE(f.valor_de_venda,0) - COALESCE(f.valor_de_compra,0)) < 0\n",
    "\"\"\")\n",
    "margem_negativa = query7.toPandas()\n",
    "if not margem_negativa.empty:\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(data=margem_negativa, x=\"margem_media\", y=\"revenda\", hue=\"produto\", palette=\"Reds_r\", dodge=False)\n",
    "    plt.title(\"Produtos/Postos com Margem Negativa\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Nenhum produto/posto com margem negativa encontrado.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "mvp_engdados",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
