{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1b0fa7f-0206-4454-acc0-6bb6f1dc59c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP carregado com sucesso.\nCSV encontrado dentro do ZIP: Preços semestrais - AUTOMOTIVOS_2025.01.csv\nColunas normalizadas: Index(['i»¿regiao_-_sigla', 'estado_-_sigla', 'municipio', 'revenda',\n       'cnpj_da_revenda', 'nome_da_rua', 'numero_rua', 'complemento', 'bairro',\n       'cep', 'produto', 'data_da_coleta', 'valor_de_venda', 'valor_de_compra',\n       'unidade_de_medida', 'bandeira'],\n      dtype='object')\nQuantidade total de registros: 429523\n\nValores ausentes por coluna:\ni»¿regiao_-_sigla      9114\nestado_-_sigla         9114\nmunicipio              9114\nrevenda                9114\ncnpj_da_revenda        9114\nnome_da_rua            9114\nnumero_rua             9172\ncomplemento          332908\nbairro                 9976\ncep                    9114\nproduto                9114\ndata_da_coleta         9114\nvalor_de_venda         9114\nvalor_de_compra      429523\nunidade_de_medida      9114\nbandeira               9114\ndtype: int64\n\nTipos de dados por coluna:\ni»¿regiao_-_sigla     object\nestado_-_sigla        object\nmunicipio             object\nrevenda               object\ncnpj_da_revenda       object\nnome_da_rua           object\nnumero_rua            object\ncomplemento           object\nbairro                object\ncep                   object\nproduto               object\ndata_da_coleta        object\nvalor_de_venda        object\nvalor_de_compra      float64\nunidade_de_medida     object\nbandeira              object\ndtype: object\n\nNúmero de linhas duplicadas: 9113\n\nEstatísticas descritivas para valor_de_venda e valor_de_compra:\n       valor_de_compra\ncount              0.0\nmean               NaN\nstd                NaN\nmin                NaN\n25%                NaN\n50%                NaN\n75%                NaN\nmax                NaN\n\nValores únicos na coluna bandeira (exemplo até 10): ['RAIZEN' 'VIBRA' 'IPIRANGA' 'BRANCA' 'ALE' 'TOTALENERGIES' 'SABBÃ\\x81'\n 'LARCO' 'TAURUS' 'PETROBAHIA'] ... total: 47\n\nValores únicos na coluna produto (exemplo até 10): ['GASOLINA' 'GASOLINA ADITIVADA' 'DIESEL S10' 'ETANOL' 'DIESEL' 'GNV' nan] ... total: 7\n\nValores únicos na coluna unidade_de_medida (exemplo até 10): ['R$ / litro' 'R$ / mÂ³' nan] ... total: 3\nCorrigindo coluna: i»¿regiao_-_sigla → regiao_sigla\nCorrigindo coluna: estado_-_sigla → estado_sigla\nColunas finais no Spark: ['regiao_sigla', 'estado_sigla', 'municipio', 'revenda', 'cnpj_da_revenda', 'nome_da_rua', 'numero_rua', 'complemento', 'bairro', 'cep', 'produto', 'data_da_coleta', 'valor_de_venda', 'valor_de_compra', 'unidade_de_medida', 'bandeira']\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================\n",
    "# IMPORTAÇÕES COMENTADAS\n",
    "# ===========================================================\n",
    "import requests                  # Para baixar arquivos da web (ZIP da ANP)\n",
    "import zipfile                   # Para abrir e ler arquivos ZIP\n",
    "import pandas as pd              # Manipulação de DataFrames\n",
    "import unicodedata               # Remover acentos e normalizar texto\n",
    "import io                        # Manipular bytes em memória (BytesIO)\n",
    "from pyspark.sql import functions as F      # Funções SQL do Spark\n",
    "from pyspark.sql.types import DoubleType    # Tipo double para colunas numéricas\n",
    "import matplotlib.pyplot as plt  # Gráficos\n",
    "import seaborn as sns            # Gráficos estilizados\n",
    "sns.set(style=\"whitegrid\")       # Estilo base\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# BAIXAR E CARREGAR OS DADOS DA ANP (ZIP → CSV)\n",
    "# ===========================================================\n",
    "url_zip = \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos/shpc/dsas/ca/ca-2025-01.zip\"\n",
    "\n",
    "response = requests.get(url_zip)\n",
    "response.raise_for_status()\n",
    "\n",
    "zip_bytes = io.BytesIO(response.content)\n",
    "print(\"ZIP carregado com sucesso.\")\n",
    "\n",
    "with zipfile.ZipFile(zip_bytes) as zip_ref:\n",
    "    csv_name = [n for n in zip_ref.namelist() if n.lower().endswith(\".csv\")][0]\n",
    "    csv_bytes = zip_ref.read(csv_name)\n",
    "\n",
    "print(f\"CSV encontrado dentro do ZIP: {csv_name}\")\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# LER CSV E NORMALIZAR COLUNAS\n",
    "# ===========================================================\n",
    "df = pd.read_csv(io.BytesIO(csv_bytes), sep=\";\", encoding=\"latin1\", low_memory=False)\n",
    "\n",
    "def normalize_column(col):\n",
    "    col = col.lower()\n",
    "    col = ''.join(c for c in unicodedata.normalize('NFD', col) if unicodedata.category(c) != 'Mn')\n",
    "    col = col.replace(\" \", \"_\")\n",
    "    return col\n",
    "\n",
    "df.columns = [normalize_column(c) for c in df.columns]\n",
    "\n",
    "# Remover BOM\n",
    "df.columns = [c.replace(\"ï»¿\", \"\") for c in df.columns]\n",
    "\n",
    "print(\"Colunas normalizadas:\", df.columns)\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# ANALISE DE QUALIDADE DE DADOS\n",
    "# ===========================================================\n",
    "\n",
    "# 0. Quantidade de registros --429.523\n",
    "print(f\"Quantidade total de registros: {df.shape[0]}\") \n",
    "\n",
    "# 1. Valores ausentes por coluna\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"\\nValores ausentes por coluna:\")\n",
    "print(missing_values)\n",
    "\n",
    "# 2. Tipos de dados\n",
    "print(\"\\nTipos de dados por coluna:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# 3. Duplicados\n",
    "duplicados = df[df.duplicated()]\n",
    "print(f\"\\nNúmero de linhas duplicadas: {duplicados.shape[0]}\")\n",
    "\n",
    "# 4. Estatísticas descritivas para valor_de_venda e valor_de_compra\n",
    "print(\"\\nEstatísticas descritivas para valor_de_venda e valor_de_compra:\")\n",
    "print(df[['valor_de_venda', 'valor_de_compra']].describe())\n",
    "\n",
    "# 5. Valores únicos para colunas categóricas principais (ajuste conforme colunas do seu df)\n",
    "categorical_cols = ['estado_sigla', 'regiao_sigla', 'bandeira', 'produto', 'unidade_de_medida']\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        unique_vals = df[col].unique()\n",
    "        print(f\"\\nValores únicos na coluna {col} (exemplo até 10): {unique_vals[:10]} ... total: {len(unique_vals)}\")\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# CONVERTER VALORES NUMÉRICOS (VENDA E COMPRA)\n",
    "# ===========================================================\n",
    "for coluna in [\"valor_de_venda\", \"valor_de_compra\"]:\n",
    "    df[coluna] = (\n",
    "        df[coluna]\n",
    "        .astype(str)\n",
    "        .str.replace(\",\", \".\", regex=False)\n",
    "    )\n",
    "    df[coluna] = pd.to_numeric(df[coluna], errors=\"coerce\")\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# CRIAR DATAFRAME SPARK\n",
    "# ===========================================================\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "# Converter novamente no Spark como segurança\n",
    "spark_df = spark_df.withColumn(\"valor_de_venda\",\n",
    "    F.regexp_replace(\"valor_de_venda\", \",\", \".\").cast(DoubleType())\n",
    ")\n",
    "spark_df = spark_df.withColumn(\"valor_de_compra\",\n",
    "    F.regexp_replace(\"valor_de_compra\", \",\", \".\").cast(DoubleType())\n",
    ")\n",
    "spark_df = spark_df.withColumn(\"data_da_coleta\",\n",
    "    F.to_date(\"data_da_coleta\", \"dd/MM/yyyy\")\n",
    ")\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# 3.1 — CORRIGIR NOMES DE COLUNA COM BOM NO SPARK\n",
    "# ===========================================================\n",
    "for col in spark_df.columns:\n",
    "    # corrigir qualquer nome contendo 'regiao'\n",
    "    if \"regiao\" in col:\n",
    "        print(f\"Corrigindo coluna: {col} → regiao_sigla\")\n",
    "        spark_df = spark_df.withColumnRenamed(col, \"regiao_sigla\")\n",
    "\n",
    "    # corrigir qualquer nome contendo 'estado'\n",
    "    if \"estado\" in col:\n",
    "        print(f\"Corrigindo coluna: {col} → estado_sigla\")\n",
    "        spark_df = spark_df.withColumnRenamed(col, \"estado_sigla\")\n",
    "\n",
    "\n",
    "print(\"Colunas finais no Spark:\", spark_df.columns)\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# DIMENSÃO TEMPO\n",
    "# ===========================================================\n",
    "dim_tempo = (\n",
    "    spark_df\n",
    "    .select(\"data_da_coleta\").distinct()\n",
    "    .withColumn(\"ano\", F.year(\"data_da_coleta\"))\n",
    "    .withColumn(\"mes\", F.month(\"data_da_coleta\"))\n",
    "    .withColumn(\"dia\", F.dayofmonth(\"data_da_coleta\"))\n",
    "    .withColumnRenamed(\"data_da_coleta\", \"data\")\n",
    "    .withColumn(\"tempo_id\", F.monotonically_increasing_id())\n",
    ")\n",
    "\n",
    "dim_tempo.createOrReplaceTempView(\"dim_tempo\")\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# DIMENSÃO PRODUTO\n",
    "# ===========================================================\n",
    "dim_produto = (\n",
    "    spark_df\n",
    "    .select(\"produto\", \"unidade_de_medida\").distinct()\n",
    "    .withColumn(\"produto_id\", F.monotonically_increasing_id())\n",
    ")\n",
    "\n",
    "dim_produto.createOrReplaceTempView(\"dim_produto\")\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# DIMENSÃO POSTO\n",
    "# ===========================================================\n",
    "dim_posto = (\n",
    "    spark_df.select(\n",
    "        \"cnpj_da_revenda\", \"revenda\", \"nome_da_rua\", \"numero_rua\",\n",
    "        \"complemento\", \"bairro\", \"cep\", \"municipio\",\n",
    "        \"estado_sigla\", \"regiao_sigla\", \"bandeira\"\n",
    "    )\n",
    "    .distinct()\n",
    "    .withColumnRenamed(\"cnpj_da_revenda\", \"posto_id\")\n",
    ")\n",
    "\n",
    "dim_posto.createOrReplaceTempView(\"dim_posto\")\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# FATO PREÇOS\n",
    "# ===========================================================\n",
    "fato_precos = (\n",
    "    spark_df.alias(\"df\")\n",
    "    .join(dim_posto.alias(\"po\"), F.col(\"df.cnpj_da_revenda\") == F.col(\"po.posto_id\"), \"left\")\n",
    "    .join(dim_produto.alias(\"pr\"), F.col(\"df.produto\") == F.col(\"pr.produto\"), \"left\")\n",
    "    .join(dim_tempo.alias(\"t\"), F.col(\"df.data_da_coleta\") == F.col(\"t.data\"), \"left\")\n",
    "    .select(\n",
    "        F.col(\"po.posto_id\"),\n",
    "        F.col(\"pr.produto_id\"),\n",
    "        F.col(\"t.tempo_id\"),\n",
    "        F.col(\"df.valor_de_venda\"),\n",
    "        F.col(\"df.valor_de_compra\")\n",
    "    )\n",
    ")\n",
    "\n",
    "fato_precos.createOrReplaceTempView(\"fato_precos\")\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# QUERY 1 — POSTOS MAIS RENTÁVEIS\n",
    "# ===========================================================\n",
    "query1 = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        po.revenda,\n",
    "        SUM(COALESCE(f.valor_de_venda,0) - COALESCE(f.valor_de_compra,0)) AS lucro_total,\n",
    "        AVG(COALESCE(f.valor_de_venda,0) - COALESCE(f.valor_de_compra,0)) AS margem_media\n",
    "    FROM fato_precos f\n",
    "    JOIN dim_posto po ON f.posto_id = po.posto_id\n",
    "    GROUP BY po.revenda\n",
    "\"\"\")\n",
    "\n",
    "top_postos = query1.toPandas()\n",
    "top_postos[\"lucro_total\"] = top_postos[\"lucro_total\"].astype(float)\n",
    "top_postos = top_postos.sort_values(\"lucro_total\", ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=top_postos, x=\"lucro_total\", y=\"revenda\", palette=\"viridis\")\n",
    "plt.title(\"Top 10 Postos Mais Rentáveis\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# ESTADO MAIS RENTÁVEL\n",
    "# ===========================================================\n",
    "query_estado = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        po.estado_sigla AS estado,\n",
    "        SUM(COALESCE(f.valor_de_venda,0) - COALESCE(f.valor_de_compra,0)) AS lucro_total\n",
    "    FROM fato_precos f\n",
    "    JOIN dim_posto po ON f.posto_id = po.posto_id\n",
    "    GROUP BY po.estado_sigla\n",
    "    ORDER BY lucro_total DESC\n",
    "    LIMIT 1\n",
    "\"\"\")\n",
    "\n",
    "estado_selecionado = query_estado.toPandas().loc[0, \"estado\"]\n",
    "print(\"Estado mais lucrativo:\", estado_selecionado)\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# BAIRROS MAIS RENTÁVEIS DO ESTADO\n",
    "# ===========================================================\n",
    "query_bairro = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        po.bairro,\n",
    "        SUM(COALESCE(f.valor_de_venda,0) - COALESCE(f.valor_de_compra,0)) AS lucro_total,\n",
    "        AVG(COALESCE(f.valor_de_venda,0) - COALESCE(f.valor_de_compra,0)) AS margem_media\n",
    "    FROM fato_precos f\n",
    "    JOIN dim_posto po ON f.posto_id = po.posto_id\n",
    "    WHERE po.estado_sigla = '{estado_selecionado}'\n",
    "    GROUP BY po.bairro\n",
    "    ORDER BY lucro_total DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "bairros_lucrativos = query_bairro.toPandas()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=bairros_lucrativos, x=\"lucro_total\", y=\"bairro\", palette=\"viridis\")\n",
    "plt.title(f\"Bairros Mais Rentáveis em {estado_selecionado}\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# LUCRO POR BANDEIRA\n",
    "# ===========================================================\n",
    "query4 = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        po.bandeira,\n",
    "        SUM(COALESCE(f.valor_de_venda,0) - COALESCE(f.valor_de_compra,0)) AS lucro_total\n",
    "    FROM fato_precos f\n",
    "    JOIN dim_posto po ON f.posto_id = po.posto_id\n",
    "    GROUP BY po.bandeira\n",
    "\"\"\")\n",
    "\n",
    "bandeiras = query4.toPandas().sort_values(\"lucro_total\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(data=bandeiras, x=\"lucro_total\", y=\"bandeira\", palette=\"coolwarm\")\n",
    "plt.title(\"Lucro Total por Bandeira\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# PRODUTOS — PREÇO x MARGEM\n",
    "# ===========================================================\n",
    "query5 = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        p.produto,\n",
    "        AVG(COALESCE(f.valor_de_venda,0)) AS preco_medio,\n",
    "        AVG(COALESCE(f.valor_de_venda,0) - COALESCE(f.valor_de_compra,0)) AS margem_media\n",
    "    FROM fato_precos f\n",
    "    JOIN dim_produto p ON f.produto_id = p.produto_id\n",
    "    GROUP BY p.produto\n",
    "\"\"\")\n",
    "\n",
    "produtos = query5.toPandas()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(data=produtos, x=\"preco_medio\", y=\"margem_media\", hue=\"produto\", s=120)\n",
    "plt.title(\"Preço Médio x Margem Média por Produto\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# AUMENTOS PERCENTUAIS DE PREÇOS\n",
    "# ===========================================================\n",
    "query6 = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        po.revenda,\n",
    "        p.produto,\n",
    "        MIN(COALESCE(f.valor_de_venda,0)) AS preco_min,\n",
    "        MAX(COALESCE(f.valor_de_venda,0)) AS preco_max,\n",
    "        ( (MAX(COALESCE(f.valor_de_venda,0)) - MIN(COALESCE(f.valor_de_venda,0)))\n",
    "            / NULLIF(MIN(COALESCE(f.valor_de_venda,0)), 0) ) * 100 AS variacao_percentual\n",
    "    FROM fato_precos f\n",
    "    JOIN dim_posto po ON f.posto_id = po.posto_id\n",
    "    JOIN dim_produto p ON f.produto_id = p.produto_id\n",
    "    GROUP BY po.revenda, p.produto\n",
    "\"\"\")\n",
    "\n",
    "variacoes = query6.toPandas().sort_values(\"variacao_percentual\", ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=variacoes, x=\"variacao_percentual\", y=\"revenda\", hue=\"produto\", dodge=False)\n",
    "plt.title(\"Top 10 Maiores Aumentos de Preço\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# MARGEM NEGATIVA\n",
    "# ===========================================================\n",
    "query7 = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        po.revenda,\n",
    "        p.produto,\n",
    "        AVG(COALESCE(f.valor_de_venda,0)) AS preco_medio,\n",
    "        AVG(COALESCE(f.valor_de_compra,0)) AS custo_medio,\n",
    "        AVG(COALESCE(f.valor_de_venda,0) - COALESCE(f.valor_de_compra,0)) AS margem_media\n",
    "    FROM fato_precos f\n",
    "    JOIN dim_posto po ON f.posto_id = po.posto_id\n",
    "    JOIN dim_produto p ON f.produto_id = p.produto_id\n",
    "    GROUP BY po.revenda, p.produto\n",
    "    HAVING AVG(COALESCE(f.valor_de_venda,0) - COALESCE(f.valor_de_compra,0)) < 0\n",
    "\"\"\")\n",
    "\n",
    "margem_negativa = query7.toPandas()\n",
    "\n",
    "if not margem_negativa.empty:\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(data=margem_negativa, x=\"margem_media\", y=\"revenda\", hue=\"produto\", palette=\"Reds_r\", dodge=False)\n",
    "    plt.title(\"Produtos/Postos com Margem Negativa\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Nenhum produto/posto com margem negativa encontrado.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "mvp_engdados",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
